---
title: 'TP 4: Customer Rating of Breakfast Cereals'
author: "Emily Schmidt"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
library(data.table) # Fast aggregation of large data sets and a fast file reader
library(lubridate) # Commute date-times
library(ggplot2) # Provides helpful commands to create complex plots from data in a data frame
library(reshape2) # Easily transform our data into whatever structure we may need
library(dplyr) # Data manipulation operations such as applying filter
library(tidyr) # Contains tools for changing the shape and hierarchy of a data set
library(caret) # Functions that attempt to streamline the process for creating predictive models
library(RColorBrewer) # Tool to manage colors 
library(heatmaply) # Used to interactively visualize the data before and after transformation
library(plotly) # Create interactive, publication-quality graphs
library(factoextra) # Flexible and easy-to-use methods to extract quickly
library(modeest) # Provides estimators of the mode of univariate unimodal data and values of the modes of usual probability distributions
library(naniar) # Missing values analysis
library(kableExtra) # Create tables
library(cluster) # Use agnes() to find the agglomerative coefficient
library(reshape2) # melt() function
```

## Problem 15.3
### Preliminary Data Exploratory

```{r, message=FALSE, warning=FALSE}
setwd("C:\\Users\\emann\\GSEM Master\\Data Mining\\Homework 4") # Set directory
getwd() # Check the working directory

Cereal <- read.csv("Cereals.csv", sep = ",", header = T) # Load your data, Cereals.csv

# Add Cereal to row names (page 362)
row.names(Cereal) <- Cereal$name

# Remove 'name' column, the rows are now 'name' (page 362)
Cereal = Cereal[,-1]

```

```{r, message=FALSE, warning=FALSE}
dim(Cereal) # Dimensions of data
colnames(Cereal) # List of all variables
head(Cereal) # Display the first six rows 
t(t(sapply(Cereal, function(x) length(unique(x))))) # Number of unique values in each variable
summary(Cereal) # Produce result summaries of all variables
str(Cereal) # Compactly displaying the internal structure of a R object
```

#### Quick Summary - Part 1

There are 77 observations and 15 variables since 'name' was removed due to row names now being the cereal brand. The information contains details on rating, shelf height, product name, and other cereal features. When calling summary(), it is noticed that there are missing values and various types of variables (chr, int, and num).

Quantitative variables:\
-   calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, weight, cups, and rating\

Categorical variables (N/O):\
-   N: name (now not an included variable), mfr, and type\
-   O: shelf\

#### Missing Values Analysis

```{r, message=FALSE, warning=FALSE}
which(is.na(Cereal)) # Function returns the positions with missing values
sum(is.na(Cereal))  # Sum the total amount of missing values in the data set

# Visualize missing values
gg_miss_var(Cereal) + ggtitle("Missing values")

Cereal <- na.omit(Cereal) # Remove the four missing values, now 74 observations
```

There are four missing values that come from 'potass' (2), 'sugar' (1), and 'carbo' (1). Since we need to omit any NA values, this has been done with the na.omit() function. Since we remove those rows, we know have 74 observations since one observation has two missing values.

#### Quick Summary - Part 2

```{r, message=FALSE, warning=FALSE}
# Create a summary statistics table to show metrics, ensuring that missing values are not included
Summary <- data.frame(mean = sapply(Cereal[,c(3:11, 13:15)], mean,na.rm = T) 
                ,median = sapply(Cereal[,c(3:11, 13:15)], median,na.rm = T)
                ,min = sapply(Cereal[,c(3:11, 13:15)], min,na.rm = T)
                ,max = sapply(Cereal[,c(3:11, 13:15)], max,na.rm = T)
                ,sd = sapply(Cereal[,c(3:11, 13:15)], sd,na.rm = T))
colnames(Summary) = c("Mean","Median","Min","Max","Standard Deviation")
rownames(Summary) <- names(Cereal)[c(3:11, 13:15)] # Rename rows
kable(Summary) %>% kable_classic() # Final summary data frame

```

```{r, message=FALSE, warning=FALSE}
Cereal_QA <- Cereal[,c(3:11, 13:15)] # Create data frame with numerical variables

Cereal_QA %>% # Plot hist for all variables except "shelf"
  gather() %>%  # Convert to key (names of the original columns) & value (data held in columns) pairs
  ggplot(aes(x = value)) +                   
  geom_histogram(aes(y = ..density..), color = "black", fill = "lightgreen") +  # Add histogram, scale y-axis
  geom_density(alpha = 0.5, fill = "grey") + # Add density curve      
  facet_wrap(~ key, scales = "free") +  # In separate panels
  theme_minimal()
```

Between the summary statistics table and the histograms, there are several observations that are noteworthy:\
- All of the quantitative variables have different scales. Prior to beginning our exercise, it is important to normalize every variable to ensure that our clustering efforts are in good measure because 'it is customary to normalize continuous measurements before computing the Euclidean distance' (page 362). This can be seen completed  momentarily.\
- 'sodium' and 'potass' have the largest variability.\
- 'potass' and 'rating' appear to be positively skewed while 'carbo' is skewed left (negative).\
- There are several outliers within the variables such as 'fat' having an extreme value at 5, 'vitamins' at 100, and 'rating' at 93.7.  

We continue to explore our data by reviewing the raw data for every cereal brand. As mentioned above, the scaling ranges from 0 to 300 due to 'potass' and 'sodium.' For any continuous measurements, those variables with larger scales have much more influence over the total distance (page 362).
```{r,fig.width = 10, fig.height = 10}
heatmaply(
  Cereal_QA,
  xlab = "Features",
  ylab = "Cereals", 
  main = "Raw Data"
)
```

Normalizing the cereal data though will allow equal weights throughout all variables. Now instead of a wide scale, the values now are contained between 0 and 1.
```{r,fig.width = 10, fig.height = 10}
heatmaply(
  normalize(Cereal_QA),
  xlab = "Features",
  ylab = "Cereals", 
  main = "Data Normalization"
)
```

- **Resource:** <https://www.datanovia.com/en/blog/how-to-normalize-and-standardize-data-in-r-for-great-heatmap-visualization/>

**A.** *Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Compare the dendrograms from single linkage and complete linkage, and look at cluster centroids. Comment on the structure of the clusters and on their stability. Hint: To obtain cluster centroids for hierarchical clustering, compute the average values of each cluster members, using the aggregate() function.*  

The data is normalized between a 0 to 1 scale.
```{r,fig.width = 13, fig.height = 8, message=FALSE, warning=FALSE}
# Normalize data using preProcess() to normamlize all data (page 177)
cereals_data = preProcess(Cereal, method=c("range"))
norm.df = predict(cereals_data, Cereal)

# Compute euclidean distance (page 362)
d.norm <- dist(norm.df, method = "euclidean")

# Use hclust() to set argument method (page 372) OR compute with agnes() to find certain hierarchical clustering 
single.clust <- agnes(d.norm, method = "single")

single.clust$ac # Check agglomerative coefficient

pars <- par()
# Plot the dendrogram (page 372)
plot(single.clust, hang = -1, ann = F, xlab = "", ylab = "Euclidean Distance", sub = "", 
     main = "Single Link Cluster Dendrogram", lwd = 2)

par(lwd=2, mar=c(0,0,0,0))

# Retroactively added borders around the 'best' k value at 9
rect.hclust(single.clust, k = 9, border = 2:10)
```

Single linkage clustering is one method of hierarchical clustering. It is the similarity of two clusters of their most similar members (brands of cereal). The single-link merge is considered local, and the area where the two clusters come closest is what is compared. Additionally, it is worth mentioning that there are drawbacks with using this method. For example, this type of clustering 'is rather prone to chaining (also known as space-contracting), which is the tendency for newly formed clusters to move closer to individual observations, so observations end up joining other clusters rather than another individual observation. We can see this markedly in the hierarchical cluster plot above. The single linkage method is also sensitive to errors in distances between observations.' The first chart is about the agglomerative coefficient because this allows us to find certain hierarchical clustering methods that can identify strong clustering structures. That value is 62.8% which will be compared to the 'complete' linkage shortly.

**Resources:**  

- https://nlp.stanford.edu/IR-book/html/htmledition/single-link-and-complete-link-clustering-1.html  
- https://rpubs.com/aaronsc32/hierarchical-clustering-single-linkage-algorithm  
- https://uc-r.github.io/hc_clustering  

```{r,fig.width = 13, fig.height = 8, message=FALSE, warning=FALSE}
# Compute euclidean distance (page 362)
d.norm <- dist(norm.df, method = "euclidean")

# Use hclust() to set argument method (page 372) OR compute with agnes() to find certain hierarchical clustering 
comp.clust <- agnes(d.norm, method = "complete")

comp.clust$ac # Check agglomerative coefficient

pars <- par()
# Plot the dendrogram (page 372)
plot(comp.clust, hang = -1,  xlab = "", ylab = "Euclidean Distance", sub = "", 
     main = "Complete Link Cluster Dendrogram", lwd = 2)

par(lwd=2, mar=c(0,0,0,0))

# Retroactively added borders around the 'best' k value at 9
rect.hclust(single.clust, k = 9, border = 2:10)
abline(h = 1.48, lty = 3, col = "blue")
```

Another type of hierarchical clustering is complete. 'In complete linkage clustering, the distance between two clusters is the maximum distance' (page 370). This means that the observations are compared between the farthest pair of records. It typically means that these dendrograms will show more compact clusters. When comparing the coefficients, the complete linkage shows that its structure is stronger in terms of clustering at 83.1%. Since we are measuring the farther records, it makes sense that we have a more separated structure since that points are not comparing what is nearest. This method too is also easier to visually read/group the cereals into a certain cluster. This can be seen by the colored borders that I retroactively added after I calculated the optimal k to show how these are broken up by their distance.

*Additional comment*: When trying to optimize the k, it was between 2 and 9. 2 seemed too low so 9 was what was used throughout this analysis.  

This graph is a little messy, but this function plots the two dendrograms side by side their their labels connected by lines. It displays “unique” nodes, with a combination of labels/items not present in the other tree, highlighted with dashed lines. The quality of the alignment of the two trees can be measured using the function entanglement. A lower entanglement coefficient corresponds to a good alignment. 
```{r,message=FALSE, warning=FALSE}
library(dendextend)
# Compute distance matrix
res.dist <- dist(norm.df, method = "euclidean")

# Compute 2 hierarchical clusterings
hc1 <- hclust(res.dist, method = "complete")
hc2 <- hclust(res.dist, method = "single")

# Create two dendrograms
dend1 <- as.dendrogram (hc1)
dend2 <- as.dendrogram (hc2)

dend_list <- dendlist(dend1, dend2)

tanglegram(dend1, dend2,
  highlight_distinct_edges = FALSE, # Turn-off dashed lines
  common_subtrees_color_lines = FALSE, # Turn-off line colors
  common_subtrees_color_branches = TRUE, # Color common branches 
  main = paste("Entanglement =", round(entanglement(dend_list), 2))
  )
```

**Resource:** https://uc-r.github.io/hc_clustering

```{r,message=FALSE, warning=FALSE}
# Elbow Method
fviz_nbclust(norm.df, FUN = hcut, method = "wss")

# Average Silhouette Method
fviz_nbclust(norm.df, FUN = hcut, method = "silhouette")
```

In this plot, it appears that there is an “elbow” or bend at what appears to be k = 2 clusters. This is the point where the total within sum of squares begins to level off. This tells us that the optimal number of clusters to use in the k-means algorithm is 2 which seems very low. Another approach reviewed was the silhouette method that gives an optimal k = 9. Therefore, the best k used was 9 because when looking at the dendrogram, it can be distinctly seen where the best k is located. Please note that there are multiple way to calculate the optimal k. If using a different method, the best k may not be equal to what is represented in this report.

**Resource:** https://www.r-bloggers.com/2017/02/finding-optimal-number-of-clusters/

Create a graphic to look at single linkage centroids  by k = 9.
```{r,fig.width = 13, fig.height = 7, message=FALSE, warning=FALSE}
# Cut the single linkage tree into nine groups
optimal_single = cutree(single.clust, k = 9)

# Compute average values using aggregate(), shelf not included since it is not quantitative
single_cent <- aggregate(norm.df[, c(3:11,13:15)], by = list(optimal_single), FUN = mean)

# Return matrix for single centroid
single_cent_matrix = as.matrix(single_cent[, -1])

# Plot an empty scatter plot (page 380)
plot(c(0), xaxt = 'n', ylab = "", xlab = "", type = "l", ylim = c(min(single_cent_matrix), max(single_cent_matrix)), xlim = c(0, 13)) # Extend past variable # to add labels to both sides

# Label x-axes (page 380)
axis(1, at = c(1:12), labels = names(norm.df[, c(3:11,13:15)])) 

# Plot centroids (page 380) 
for(i in c(1:9))
  lines(single_cent_matrix[i,], lty = i, lwd = 2, col = ifelse(i %in% c(1,3,5,7,9), "blue", "grey"))

# Name clusters (page 380)
text(x = 0.2, y = single_cent_matrix[1:5,1], labels = paste("Cluster", c(1:5)))

# Name clusters (page 380)
text(x = 13, y = single_cent_matrix[6:9,12], labels = paste("Cluster", c(6:9)))
```

Create a graphic to look at complete linkage centroids  by k = 9.
```{r,fig.width = 13, fig.height = 7, message=FALSE, warning=FALSE}
# Cut the single linkage tree into nine groups
optimal_comp = cutree(comp.clust, k = 9)

# Compute average values using aggregate(), shelf not included since it is not quantitative
comp_cent <- aggregate(norm.df[, c(3:11,13:15)], by = list(optimal_comp), FUN = mean)

# Return matrix for single centroid
comp_cent_matrix = as.matrix(comp_cent[, -1])

# Plot an empty scatter plot (page 380)
plot(c(0), xaxt = 'n', ylab = "", xlab = "", type = "l", ylim = c(min(comp_cent_matrix), max(comp_cent_matrix)), xlim = c(0, 13)) # Extend past variable # to add labels to both sides

# Label x-axes (page 380)
axis(1, at = c(1:12), labels = names(norm.df[, c(3:11,13:15)])) 

# Plot centroids (page 380) 
for(i in c(1:9))
  lines(comp_cent_matrix[i,], lty = i, lwd = 2, col = ifelse(i %in% c(1,3,5,7,9), "blue", "grey"))

# Name clusters (page 380)
text(x = 0.2, y = comp_cent_matrix[1:5,1], labels = paste("Cluster", c(1:5)))

# Name clusters (page 380)
text(x = 13, y = comp_cent_matrix[6:9,12], labels = paste("Cluster", c(6:9)))
```


**B.** *Which method leads to the most insightful or meaningful clusters?*

The overall goal of clustering is to find distinct groups or "clusters" within a data set. For this exercise, cereals were divided among their euclidean distance whether that was by single or complete linkage. Therefore, it can be concluded that the most insightful cluster would be the complete method. Then reviewing the centroids, it can be seen that each 'path' appears to be independent of one another.  

**C**. *Choose one of the methods. How many clusters would you use? What distance is used for this cutoff? (Look at the dendrogram.)* 
The chosen method would be complete. From the optimal methods above, I chose k = 9 throughout my analysis. The distance used for the cutoff appears to between 1.48 and 1.49 when eyeballing the dendrogram. Therefore, I added a dotted blue line in the complete linkage graphic that can be referenced above.  

**D.** *The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?*

```{r,fig.width = 8, fig.height = 4, message=FALSE, warning=FALSE}
# Rename grouping of clusters 
comp_cent <- comp_cent %>% 
                    rename("Cluster" = "Group.1")

# Plot each cluster based off their grouping using melt() and ggplot()
comp_cent.melt <- melt(data = comp_cent, id.vars = "Cluster")

ggplot(comp_cent.melt, aes(x = variable, y = value, color = factor(Cluster), group = Cluster, size = 1.5)) +
  xlab("Cereal Attribute") +
  ggtitle("Cereal Clustering") +
  ylab("Cluster") +
  labs(color = "Group") +
  geom_line(size = 0.8) +
  geom_hline(yintercept = 0, linetype="dashed", color = "black") +
  geom_hline(yintercept = 0.1, linetype="dashed", color = "black") +
  theme_classic()

# Call which cereals are in the respective cluster
I9 <- which(optimal_comp == 9)

kable(I9) %>% kable_classic() # Final summary data frame
```

Since the elementary school wishes to choose the healthiest cereals, there are several cereal features that should be focused on.  To be considered a 'healthy' cereal, it should not have a lot of 'sugar', 'sodium', 'fat', or 'calories.' Please note that the version of a healthy cereal is in my opinion and not that of a cereal expert (SME). Therefore, if those four attributes are focused on and expected to be low, it appears that cluster 9 fall into those categories (seen between black dashed lines). These variables should continue to be normalized because they are continuous and to get the best estimate, all weighting should be equal even though in this case we are saying they should not be. Within cluster 9 (2 brands), we have the following 2 cereals that would be recommended to the school: 

- Puffed_Rice 
- Puffed_Wheat

If the school wanted more than two options, clusters 5 and 7 could have good brands to give the children as they fall between those black lines indicating that a cereal attribute is low for that brand. 
